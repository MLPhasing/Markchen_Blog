# CS188 Final Presentation
> By Mark & Sirius

## 什么是机器学习？
**机器学习简单来说就是让计算机程序通过输入的信息和做出决策后得到的反馈自动调整对不同输入的对策的学科**

从上世纪末“机器学习”第一次被提出到现在，机器学习的研究重点从一开始的“推理”（符号推理系统）到“知识”（专家系统），再到现在的“学习”（贝叶斯网络，概率学习etc）已经经历了三次变化。目前，机器学习的大部分算法都是通过使用给定的训练数据（输入信息&对应的正确输出）来“学习”到如何在给定的输入下反馈正确的输出。也就是说，我们可以把一个机器学习模型看作这样一个映射：
$$\text{Input information}\rightarrow \text{Output (Decision)}$$

> Example: 模型对MNIST手写数字集的数字识别可以看作是一个将28\*28矩阵映射到一个10维向量的过程，通过将输入的图片（28\*28矩阵）进行卷积等计算最后得到一个1*10的向量，其中第i项表示这个数字是i的概率

## 什么是深度学习
从上面的映射关系，我们可以发现实际上决策系统可以被看作一个函数，这个函数将输入信息映射到做出的决策上面。如果一个函数对每一个输入都可以映射到相对应的*最优决策*，那么这样的一个模型可以说是一个完美的决策模型。

深度学习充分利用了“决策模型是一个映射关系”的思想。神经网络的数学基础建立在这样一个理论上：  。

*调整模型参数，结构使得模型的函数映射尽量近似于最优的信息 - 决策映射的过程一般被称作“训练”*

> Example:简单的，使用sigmoid函数作为激活函数的感知器的排列组合就可以形成近似于阶梯函数的加权输出
> ![figure 1](CS188FinalPresentation.assets/Simple%20Sigmoid%20Perceptrons.png))

那么什么可以决定模型对“最佳映射函数”的拟合程度呢？主要有两个因素：
1. 模型本身的结构与复杂度
   
    如果模型本身不够复杂(层数不够多，提取的feature map不够多，etc.)，那么最后组合成的模型本身的复杂度不够高是无法很好的拟合复杂的输入 - 决策映射函数的。

    > Example: 如果一个模型里面所有的神经元都使用了线性函数作为自己的激活函数，那么最后整个模型的输出依然会是一个线性函数，因为线性函数的叠加，迭代都依然会是线性函数。
    >
    > $f(x) = 3x + 1$,  $g(x) = 2x + 1$, $f(g(x)) = 6x + 4$，可以看到线性函数的叠加并不会使得结果得到的函数具有非线性性质

2. 模型的参数
   
    对于每一个神经元一般都有两个参数：权重(wieght. $w$)和偏置(bias, $b$)；对于一个神经元来说，其激活函数的性质很大程度取决于这两个参数。如果原本的激活函数是$\sigma(x)$的话，那么加上了这两个参数的激活函数就会变成$\sigma(w\cdot x + b)$，这允许我们对每个神经元的基本函数进行基本的变形（平移&拉伸）。通过合适的参数组合，我们可以通过让每一个神经元都获得适当的参数来进行较为精确的拟合。

## 如何让模型自动更新参数？
如同前面所说的，对于一个给定的，足够复杂的模型结构来说，一般模型本身不会是制约模型训练的主要因素。然而，对于稍微大一些的模型，调整模型参数的难度会随着模型的规模程几何倍数增大。现在大部分模型至少拥有$1\times 10^3$个参数，很明显不可能通过人工方法来调节这些参数。

既然不可能通过人工方法来调节这些参数，有没有什么方法可以让程序自己调节模型的参数呢？ 要回答这个问题，我们要先探究一下当人在调节toy model的参数的时候主要涉及什么工作：
* 判断当前输出是不是一个好的输出
* 粗略估计向一个方向略微一个参数会造成结果的什么变化

类似的，要让模型拥有自动调节参数的能力，我们也需要让模型拥有这两个能力：判断表现好坏的能力和对参数调节状况的预测能力（e.g. 程序可以推断出调大参数a可能会让输出更好）。

要实现第一个功能，我们需要引入一种叫做“损失函数(loss function)”的概念。通过损失函数，我们可以量化的评估一个输出与给定的输出相比是否是一个“好输出”，以及这个输出“有多好”。 这样的损失函数可以这样表示：
$$L(\hat{y}, y) = L(\text{Model}(x), y)$$

\*一般来说，损失函数越大代表当前输出与正确输出之间的距离越远，当前输出越“差”

**通过引入损失函数的概念，我们成功的将“找到一个最优的模型”这样的笼统概念转化为了“找到一个函数$\text{Model}(x)$使得函数$L(\text{Model}(input), label)$的值最小”.**

### 如何让模型决定参数调节方向？
现在我们的训练目标是“找到一个函数$\text{Model}(x)$使得函数$L(\text{Model}(input), label)$的值最小”，而函数$\text{Model}(x)$实际上是由函数的参数$\theta$决定的，我们可以这样规范的表示训练目标：
$$\text{Optimal }\theta = \argmin_{\theta}{(L(\text{Model}_\theta(x), y))}$$
为了让程序可以识别到如何更新参数才能让损失函数最小化，我们需要引入**梯度下降**的概念：

对于一个损失函数，如果输入信号已经确定不变，那么唯一会使得损失函数减小的方法就是修改这个模型的参数，所以我们也可以这样表示一个损失函数：
$$L_x(\theta) = L_x(\langle w_1, w_2, \cdots, w_n, b_1, b_2, \cdots b_n\rangle)$$
对于一个多元函数，我们可以计算这个函数在一个点上的*梯度, Gradient*，梯度是一个矢量，代表着在当前位置函数**上升最快的方向**。梯度的定义如下：
$$\nabla L_x(\theta) = \langle \frac{\partial L_x}{\partial w_1} ,\frac{\partial L_x}{\partial w_2},\cdots, \frac{\partial L_x}{\partial w_n},\cdots, \frac{\partial L_x}{\partial \theta_n} \rangle$$
因为我们的目标是通过适当的参数输入使得损失函数最小化，我们应该将每一个参数向着**梯度负方向**移动。
![figure 2](CS188FinalPresentation.assets/Gradient%20Descent%2001.jpg)

从上面我们可以得知每一次参数更新可以这样表示:
$$w = w - \eta \frac{\partial L_x(\theta)}{\partial w}$$
